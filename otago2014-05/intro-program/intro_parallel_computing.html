<!DOCTYPE html><meta charset="utf-8"><meta name="description" content="introduction to parallel computing"><meta name="author" content="Mark Cheeseman"><title>Introduction to Parallel Computing</title><!-- Main title slide --><section>    <h2>Introduction to Parallel Computing</h2>    <footer> Mark Cheeseman<br>      <font size='-1'>mche807@nesi.org.nz</font>    </footer></section><!-- Outline of entire presentation --><section>    <header><font color="white">Outline</font></header>     <br><br><br>    <ol style="font-size: 35px;">       <li> Rise of parallel computing</li>         <li> Common reasons why people resist parallel computing</li>       <li> Fundamentals of parallel computing</li>       <li> Examples</li>    </ol>	</section><section>    <h2>Rise of parallel computing</h2></section><!-- Increase in availability of computing slides --><section>    <header>      <font color="white">Increased availability of supercomputing</font>    </header>       <p><br><br><br>Mainframes housed at special datacenters used to be the only game in town when it come to tackling intense 	   numerical problems.  These beasts were awesome at this work but had a couple of fundamental drawbacks:<br><br><strong>Expenses:</strong> their hardware was costly to buy and run.  They were physically big.  Skilled staff required to operate and administer them.<br><br> <strong>Usability:</strong> vendors had their own programming APIs and standards, software selection was limited.<br><br><strong>Accessibility:</strong> only a select few researchers could have access.</p>    </section><section>    <figure-fullpage>       <img src='images/3supercomputers.jpg'>    </figure-full-page></section><section>    <header>        <font color="white">Increased availability of supercomputing</font>    </header><p><br><br><br>The accessibility barrier has dropped considerably in recent years:</p>	<ul>	   <li>commodity prices have plummeted while performance has increased</li>	   <li>small and medium organizations are investing in cluster and other computational resources</li>	   <li>national computing initiatives (NeSI, XSEDE, NCI)</li>	   <li>cloud computing (public and commercial)</li>	</ul>	    <p><strong>Example</strong><br>In 2004 the Swiss National Supercomputer Centre housed 2 supercomputers: a NEC SX-5 and an 	IBM Power3 that served ~55 user projects.  NeSI, today, serves ~200 user projects.</p>  </section><section>   <header>   <font color="white">Increased availability of supercomputing</font>   </header>  <center><figure>  <img src='images/nec-sx5.jpg'>  </figure></center>   <p><br><br><br><br><br><br><br><br><br>The IBM Power3 system served 1.33 TFLOPS of performance.  The NEC SX-5 served only 65 GFLOPs.<br><br>   One can buy a new Apple Mac Pro with a theoretical max performance of 7 TFLOPS (CPUs+GPUs).</p></section><!-- Powerful commodity hardware slides --><section>   <header>   <font color="white">Powerful commodity hardware</font>   </header>  <figure>  <img src='images/Intel_CPU.jpg'>  </figure>   <p><br><br><br><strong>Processors</strong><br>   Modern CPUs have a max performance on the order 100-300 GFLOPs.  Rise in mobile and ultra-scale computing has    driven improvements in power efficiency.<br><br><strong>Networking</strong><br>Multiple high-speed network technologies exist (InfiniBand, Ethernet).  No need to rely on specialized networks from a particular vendor.</p></section><section>   <header>   <font color="white">Powerful commodity hardware</font>   </header>  <figure>  <img src='images/InsideanSSD.jpg'>  </figure>   <p><br><br><br><strong>Storage</strong><br>   High-speed and parallel filesystems now available to anyone.  Free and open-sourced filesystems available.     SSDs are having a disruptive effect on data-intensive computing.<br><br>  External connections (USB3, Thunderbolt)    allow high-speed access to large multi-terabyte storage (disks, tape).</p></section><section>    <header>        <font color="white">Powerful commodity hardware</font>    </header>     <figure><img src='images/tesla_gpu.jpg'></figure>    <p><br><br><br>Popularity in accelerator technology has risen sharply in recent years.</p>    <ol>	       <li> New software frameworks (CUDA, OpenCL) and initiatives (OpenACC) have greatly increased the 	   usability of these devices.</li>       <li> Hardware vendors have leveraged lower-priced commodity devices to engineer and manufacture 	   these specialized devices.</li>	</ol>    <p><em><font color="red">These devices exploit massive parallelization and vectorization.</font></em></p>   </section><!-- Beowulf slides --><section>    <header>    <font color="white">The Beowulf</font>    </header>    <p><br><br><br>In 1994, two NASA scientists strug together 30 identical PCs to create a scientific 	computing system they called the <em>Beowulf Cluster</em>.  The Message Passing Interface software was 	used to allow programmers to use multiple PCs to work on a single problem.  Their design formed the 	basis for the common cluster design now found worldwide.<br><br>    This was the beginning of the big push in distributed computing (and parallel computing).</p></section><section>    <figure-fullpage>       <img src='images/beowulf_cluster.jpg'>    </figure-full-page></section><!-- Moore's Law slides --><section>    <header>    <font color="white">Misintepretation of Moore's Law</font>    </header>    <p><br><br><br><strong><em>It's not a law!!</em></strong><br>    Gordon Moore observed that the maximum number of transistors that could be economically added to    integrated circuits seem to double every 2 years.  Another Intel manager, David House, suggested    that improved manufacturing practices would lower the time to 18 months.<br><br><strong><em>It refers to transistor density -not clock rate</em></strong><br>For ~42 years, the increased transistor counts meant higher clock rates and more instruction-level parallelism.  Users became accustomed to new CPUs speeding up their applications "auto-magically".</p>  </section><section>    <header>    <font color="white">Misintepretation of Moore's Law</font>    </header>    <figure>        <img style="width: 38%; height: 38%; right-margin: 10px;"src='images/multicore.jpg'>    </figure>    <p><br><br><br>By ~2003, this "free speed-up" ended:</p>    <ul>        <li>too expensive to add more transistors</li>        <li>unsustainable power requirements</li>        <li>CPU designs led to potentially massive performance hits</li>    </ul><br><p>CPU vendors adapted by introducing CPUs with multiple cores of simpler architecture.Applications had to be able to utilize these multiple cores to harness the additional power in new CPUs.<br><br>   <font color="red">Parallel computing has now become mainstream.</font></p></section><!-- Reasons against embracing parallel computing slides --><section>    <h2>Common reasons against embracing parallel computing</h2></section><section>    <header>        <font color="white">Next year's hardware will be faster</font>    </header><br><br><br>	<p>The doubling clock rate every 18 months phenomenon is long gone!</p>	<ul>        <li> vendors focusing on energy efficiency more than performance</li>    </ul>    <p>New CPUs and GPUs are getting more parallel-focused</p>	<ul>        <li> new CPUs have 4-12 cores.  # of cores will most likely increase.</li>		<li> new GPUs use thousands of threads for parallel programming</li>    </ul>	<p>New computer hardware is getting more complicated</p>	<ul>        <li> Use of accelerators (GPUs, FPGAs, Xeon Phi)</li>    </ul>	</section><section>    <header>        <font color="white">I don't have access to parallel hardware</font>    </header><br><br><br>	<p>You actually do!</p>    <ul>        <li> Most current CPUs are multi-core</li>        <li> GPU computing is a possibility for most workstations, PCs, and/or laptops</li>        <li> Many universities/labs have cluster resources</li>    </ul>		<p>Sources of free computing access:</p>    <ul>        <li> AWS Cloud Computing offers a free trial usage scheme</li>        <li> NeSI has access methods with zero cost to NZ researchers. <em>Ask me for for details!</em></li>    </ul></section><section>    <header>        <font color="white">No time/resource to rewrite my code</font>    </header><br><br>	<p>Don't re-invent the wheel!</p>	<ul>	    <li>existing libraries/frameworks may be able do the parallel work for you</li>	</ul>	<p>Writing parallel code isn't as hard as you think!</p>	<ul>	    <li>adding multi-core support is trivial with infrastructures such as OpenMP and OpenACC.</li>		<li>adding multiple CPU parallelism can be <em>a bit</em> more challenging</li>	</ul>	<p>Need to determine if the possible increases in throughput/problem size will out balance the time	needed for development.<br><br>	<font color="red">NeSI has CS specialists available to assist with any of the above</font></p></section><section>    <header>        <font color="white">It's too risky to change my workflow</font>    </header><br><br><br>	<figure>        <img style="width: 35%; height: 35%; right-margin: 10px;" src='images/jenga.jpg'>    </figure>		<p>Don't necessarily need to rewrite code</p>	<ul>	    <li>try switching to application(s) that exploit more parallelism</li>		<li>try new scheduling techniques</li>	</ul>	<p>Weigh the potential benefits</p>    <ul>       <li>time saved for more research</li>       <li>additional ensembles to enhance scientific outputs</li>       <li>address more challenging science</li>    </ul>    <p2><center><strong><font color="red">No risk, no reward</font></strong></center></p2></section><!-- Amdahl's Law slides --><section>    <h2>Fundamentals of parallel computing</h2></section><section>    <header>        <font color="white">Amdahl's Law</font>    </header>    <p><br><br><br>IBM Master Inventor, Gene Amdahl, presented a conference paper in 1967 that analytically 	explained how much a program can be sped up by adding parallelism.<br><br><em><font color="red">The speedup (S<sub>N</sub>) of a program using N processes is limited by the portion of the program that can only be performed sequentially (1-P).</font></em></p>    <center><figure-middle>      <img src='images/amdahl-eq.jpg'>    </figure-middle></center></section><section>      <figure-fullpage>        <img src='images/AmdahlsLaw.svg'>        <figcaption><font color="red">Doesn't take account of:<br>               - system and software overhead<br>               - workload imbalance</font>        </figcaption>    </figure-fullpage></section><section>    <header>        <font color="white">Amdahl's Law (Highlights)</font>    </header>    <p><br><br><br><strong>Can a program be 100% parallel?</strong><br> In theory, yes!  Such programs are said	to be <em>embarrassingly parallel</em>.  Processors work independently as little to no information is required to be	passed between them.  However, in practise, there will still be some start-up costs that prevent a 100% pure	parallel breakup of the work.<br><br>    <strong>Bottlenecks that prevent perfect speedup</strong></p>    <ul>        <li>passing information between tasks</li>        <li>I/O</li>        <li>spawning the individual tasks</li>        <li>explicit required serialization in the program</li>    </ul></section><!-- Domain decomposition slide --><section>    <header>        <font color="white">Domain decomposition</font>    </header>	<figure>        <img style="width: 43%; height: 43%;" src='images/simple_2d_domain_decomposition.svg'>    </figure>	    <p><br><br><br><br>The most common approach in adding parallelism to an 	application.  The computational domain is split among the individual CPUs.<br><br>	Important notes:</p>    <ul>	   <li> every CPU executes the same code but on different parts of the domain</li>	   <li> no 1 CPU "sees" the entire domain. Partitioned solution space</li>	   <li> what happens when a CPU requires data from another CPU's part of the domain?</li>	</ul> </section><!-- Ghost Cells slides --><section>	<header>        <font color="white">Ghost cells</font>    </header>	<p><br><br><br>Assume we are solving the heat equation on the 2D domain shown on the last slide.</p>    <center><figure-middle2>	<img style="width: 25%; height: 25%; bottom-margin: 0px;" src='images/heat_eqn.png'>	</figure-middle2></center>	<p>Applying a simple finite difference approximation yields the relation.</p>	<center><figure-middle2>	<img style="width: 70%; height: 70%;" src='images/finite_heat_eqn.png'>	</figure-middle2></center>	<p>Notice that each CPU will require data points from 2 neighbouring CPUs</p> </section><section>    <header>        <font color="white">Ghost cells</font>    </header>    <figure>        <img style="width: 43%; height: 43%;" src='images/simple_2d_ghost_cells.svg'>    </figure>    <p><br><br><br>CPU2 will require data points from CPUs 1 and 4 to evaluate the finite	difference relation.  It will need to store an additional column and row of data points.  These extra 	points are known as <em>ghost cells</em>.<br><br>  		These don't form part of the computational domain for any CPU but are necessary to compute the solution.	<font color="red">The number and rate at which these cells need to be updated will affect the parallel performance	of a code.</font></p>     </section><section>      <figure-fullpage><img src='images/DomainDecompositionBEM.jpg'></figure-fullpage></section><!-- Task decomposition slide --><section>    <header><font color="white">Task decomposition</font></header>    <figure><img src='images/functional_decomp.gif'></figure>    <p><br><br><br><br>Different CPUs perform different operations on the same or different data.  	Could be simple loop-unrolling or running entirely different functions/applications on data.<br><br> 	When to use this approach:</p>	<ul>        <li> computational domain too small to be decomposed</li>        <li> code has multiple execution streams taking up significant run time</li>		<li> used in hybrid parallelism (eg. when a code uses multiple levels of parallelism)</li>   </ul></section><!-- Shared Memory Model slides --><section>    <header><font color="white">Symmetric multiprocessing (SMP)</font></header>    <figure>        <img style="width: 40%; height: 40%; bottom-margin: 10px; right-margin: 10px;" src='images/shared-memory.png'>    </figure>    <p><br><br><br>SMP is where multiple processors share a common memory block.  As the entire 	computational domain is held is this memory, all CPUs can "see" the whole domain and no 	data movement is required. Eg. ghost cells not needed.<br><br>	All modern multi-core processors (CPUs and GPUs) utilize SMP. Common software implementations 	include OpenMP and CUDA.</p></section><section>    <header><font color="white">Symmetric multiprocessing (SMP)</font></header>    <br><br><br>    <p><strong>Pros</strong></p>	<ul>       <li> Implementing parallelism is (fairly) easy</li>	   <li> Performance is good (all memory transactions are local)</li>	   <li> Supported in popular software frameworks (OpenMP, CUDA) and languages (Java)</li>	</ul>   	<p><strong>Cons</strong></p>	<ul>       <li> Parallelism limited by number of cores in CPU or threads in GPU</li>	   <li> Problem size limited by amount of physical memory CPU has</li>	</ul></section><!-- Distributed Memory Model slides --><section>    <header><font color="white">Distributed memory</font></header>    <figure>        <img src='images/distributed_memory.gif'>    </figure>    <p><br><br><br>Each CPU only has access to its own local memory. If a CPU requires data from 	another CPU's memory, it must <em>request</em> and wait for that data to be passed to it.  So ghost	cells would need to be formed and filled on each CPU.<br><br>	Clusters, like the machines NeSI operate, rely on this architecture for parallelism.</p></section><section>    <header><font color="white">Distributed memory</font></header>	    <br><br><br>    <p><strong>Pros</strong></p>	<ul>       <li> MPI (Message Passing Interface) is the standard in dealing with data transfer between CPUs.  It 	   is available on most -if not all- computing platforms.</li>	   <li> Allows one to tackle more larger problems as one has access to local memory of any CPU in cluster</li>	   <li> Allows one access to more CPUs than with a shared memory platform</li>	</ul>   	<p><strong>Cons</strong></p>	<ul>       <li> Harder to use/implement than a shared memory parallel code</li>	   <li> Performance will be less than SMP for the same # of CPUs</li>	</ul></section><!-- Numerical integration slides --><section>    <h2>Numerical Integration</h2></section><section>    <header><font color="white">Background</font></header>    <p><br><br><br><strong>GOAL</strong><br>     Determine the integral of some function between two given points.</p>    <center><figure-middle2>	<img style="width: 22%; height: 22%; bottom-margin: 0px;" src='images/integral.png'>	</figure-middle2></center>    <p><strong>METHOD</strong><br>    Integral is equal to area under function's curve between the specified endpoints. Approximate	this area by summation of a series of rectangles drawn under the function.  Higher the number of 	rectangles, greater the accuracy of the area approximation.</p></section><section>    <header><font color="white">Background</font></header>    <p><br><br><br>Notice that the rectangles method introduces some error (denoted by the red).  But 	we can decrease this error by increasing the number of rectangles (or decreasing rectangle width).</p>    <center>	    <figure-middle>	        <img style="width: 29%; height: 29%; bottom-margin: 0px;" src='images/integral_rectangles.png'>	    </figure-middle>        <figure-middle>		    <img style="width: 29%; height: 29%; bottom-margin: 0px;" src='images/integral_rectangles_higher_resolution.png'>		</figure-middle>    </center>    <p>Integral turns into a simple summation.</p>    <center><figure-middle>       <img style="width: 27%; height: 27%; bottom-margin: 0px;" src='images/area_summation.png'>    </figure-middle></center></section><section>    <header><font color="white">Serial algorithm</font></header>    <p><br><br><br>A simple algorithm would be:<br></p>    <pre><code>rec_width = (some_constant)     sum = 0.0     for n = 1,num_rectangles:         sum = sum + F(x<sub>n</sub>) * rec_width</code></pre>      <p>Key observations:</p>    <ul>       <li> most of the work concentrated in the for-loop</li>       <li> area of each rectangle can be independently computed</li>       <li> order of rectangle summation could be important*</li>    </ul>     <p><em><font color="red">*Adding a very small number to a very large number in a finite number 	system can lead to inaccuracy!</font></em></p></section><section>    <header><font color="white">Adding parallelism</font></header>    <p><br><br><br>Break the global sum into series of partial sums that are computed on 	individual CPUs.</p>    <center><figure-middle2>       <img style="width: 70%; height: 70%;" src='images/partial_sums_eqn.png'>    </figure-middle2></center>      <p style="margin-top: 10px;"><strong>i<sub>1</sub>-i<sub>2</sub></strong> is the range of rectangles on which	each CPU works.</p>	<center><figure-middle2>       <img style="width: 35%; height: 35%; bottom-margin: 0px;" src='images/integral_rectangles_higher_resolution_cpu_division.png'>    </figure-middle2></center></section><section>    <header><font color="white">Parallel algorithm</font></header>	<br><br>    <p>Start by initializing some required constants and variables</p>    <pre><code><font color="red">rec_width</font> = (some_constant)     <font color="blue">sum</font> = 0.0     <font color="red">num_rectangles_per_cpu</font> = <font color="red">num_rectangles</font> / <font color="red">num_cpu</font></code></pre> 		<p>Now each CPU determines its own partial sum</p>    <pre><code>for <font color="blue">n</font> = 1 to <font color="red">num_rectangles_per_cpu</font>:         if (on CPU1) j = <font color="blue">n</font>         if (on CPU2) j = <font color="blue">n</font> + <font color="red">num_rectangles_per_cpu</font>         if (on CPU3) j = <font color="blue">n</font> + 2*<font color="red">num_rectangles_per_cpu</font>         if (on CPU4) j = <font color="blue">n</font> + 3*<font color="red">num_rectangles_per_cpu</font>         <font color="blue">sum</font> = <font color="blue">sum</font> + F(x<sub>j</sub>) * <font color="red">rec_width</font></code></pre></section><section>    <header><font color="white">Parallel algorithm</font></header>	<br><br>        <p>CPU1 gathers the partial sums from the other CPUs to determine the global value 	of the integral.</p>    <pre><code>if (on CPU1):	 <font color="blue">global_sum</font> = <font color="blue">sum</font>         for n = 2 to <font color="red">num_cpu</font>:	     get value of <font color="blue">sum</font> from CPU n             <font color="blue">global_sum</font> = <font color="blue">global_sum</font> + <font color="blue">sum</font></code></pre>	<p><strong>Observation points</strong></p>	<ul>	   <li>partial sum collection loop is a potential performance bottleneck</li>	   <li>possible accuracy problem as it as assumed that all sum values have the same order of magnitude</li>	</ul></section><section>    <header><font color="white">Variable scope</font></header>    <p><br><br><br><strong>Local Variables</strong><br>	Variables that are defined on every CPU <em>but</em> can have different values on the individual 	CPUs.  A CPU doesn't know the value of another CPU's local variable. (Software interfaces such as 	OpenMP and MPI allow a CPU to "request" the value however).  <font color="blue">sum</font> is an	example of a local variable.<br><br>		<strong>Global Variables</strong><br>	Variables that are defined and have the same value on all CPUs.  <font color="red">global_sum</font>  	and <font color="red">num_rectangles_per_cpu</font> are global variables.  <br><br>		<font="orange"><strong>A common mistake is not realizing the proper variable scope.</strong></font></p></section><!-- VOLTERRA slides --><section>    <h2>VOLTERRA Code</h2></section><section>    <header><font color="white">Background</font></header>    <p><br><br><br><strong>GOAL</strong><br>     Predict the in-situ electric field (E) in a volume of dielectric material of arbitrary size 	and shape that is bombarded by an incident EM pulse.<br><br>    <strong>METHOD</strong><br>    Dielectric volume has no initial E field. As the pulse passes through the volume, it induces an E field.  	The total electric field at any given point is</p>	<center><strong>E<sub>total</sub>=E<sub>pulse</sub>+E<sub>scatter</sub></strong></center>	</section><section>    <figure-fullpage>       <img src='images/red_blood_cell_60um.png'>    </figure-full-page>	</section><section>    <header><font color="white">Background</font></header>    <p><br><br><br>E<sub>scatter</sub> is the induced E between different points that have experienced the pulse.	The full volume integral equation describing the E field evolution is</p>	<center><figure-middle>       <img style="width: 55%; height: 55%; bottom-margin: 0px;" src='images/tdvie_eqn.png'>    </figure-middle></center>		<p><strong>R</strong> is the distance between the point being evaluated and the points that have already 	experienced the incident pulse.<br><br>	<font color="red">This a form of the classical N-body problem</font></p>	</section><section>    <header><font color="white">Serial code</font></header>    <p><br><br><br>A predictor-corrector approach to solve the integral equation.</p>	<pre><code>for 1 to num_timesteps:       for each point in volume:           evaluate integral with E<sub>TOTAL</sub> from last timestep           create updated E<sub>TOTAL</sub> value           re-evaluate integral term with updated E<sub>TOTAL</sub>           correct E<sub>TOTAL</sub> as final value for this timestep</code></pre>	<p>Integral is approximated by a sum over all points in volume.</p>	<center><figure-middle>       <img style="width: 59%; height: 59%; bottom-margin: 0px;" src='images/tdvie_sum_eqn.png'>    </figure-middle></center>	</section><section>    <header><font color="white">Adding parallelism</font></header>	<figure2><img src='images/dielectric_volume.png'></figure2>    <p><br><br><br>Key observations:</p>	<ul>	    <li>work concentrated inside a loop</li>		<li>summation order not important</li>		<li>similar domain decomposition approach as with numerical integration example		can be applied</li>	</ul>	<p>Volume is subdivided into a number of grid cells.  These grid cells are gathered into	regions that are allocated to individual CPUs. Each CPU is responsible for evaluating the 	integral for only the points it "owns".</p>	</section><section>    <header><font color="white">Adding parallelism</font></header>	<p><br><br><br>The most time-consuming part is the evaluation of the N-body summation.	Break it into a series of partial sums.</p>		<center><figure-middle>       <img style="width: 85%; height: 85%; bottom-margin: 0px;" src='images/tdvie_partial_sums.png'>    </figure-middle></center>		<p>Idea is for each CPU to determine all in-situ E field values for grid cells it "owns" with	respect to all other grid cells.</p>	<ul>	   <li>no need for the entire domain to be formed on any one CPU</li>	   <li>CPUs will need to know the global location of all grid cells</li>	</ul>		</section><section>    <header><font color="white">Parallel code</font></header>    <br><br><p>Start by setting global constants -including a mapping array that 	holds the grid cell locations.</p>		<pre><code><font color="red">mapping</font> = <em>array of locations of all grid cells</em>     <font color="blue">integral</font> = 0.0     for n = 1 to <font color="red">num_cpus</font></code></pre>     <p>Construct partial sum for grid cells belonging to CPU n</p>		<pre><code>    <font color="red">i1,i2,j1,j2,k1,k2</font> = global ranges of grid cells on CPU n         for i = <font color="red">i1</font> to <font color="red">i2</font>:         for j = <font color="red">j1</font> to <font color="red">j2</font>:         for k = <font color="red">k1</font> to <font color="red">k2</font>:             <font color="blue">integral(i,j,k)</font> = interaction between grid cells on		       local CPU and global point(i,j,k)</code></pre></section><section>    <header><font color="white">Parallel code</font></header>    <br><br><p>Need to add the partial sums to get the total integral for 	the grid cells on CPU n</p>		<pre><code>    for p = 1 to <font color="red">num_cpus</font> (p /= n):             <font color="blue">integral</font>(CPU n) = <font color="blue">integral</font>(CPU n) + <font color="blue">integral</font>(CPU p)</code></pre>							  	<p><strong>Notice:</strong><br> there's a serial bottleneck as one needs to loop over all CPUs to update 		each CPU's section of the global domain.</p>	</section><section>      <figure-fullpage><img src='images/volterra_global_reduce2.png'></figure-fullpage></section><section>    <header><font color="white">Performance</font></header>    <br><br>    <center><figure-middle>    <img style="width: 75%; height: 75%; bottom-margin: 0px;" src='images/volterra_scaling.png'>    </figure-middle></center>    <p>Results of running parallel code on an IBM Blue Gene/P at 3 different total grid cell counts.  As # of CPUs doubles, run time should be halved ideally (blue bars)</p></section><!-- UM slides --><section>    <h2>Unified Model</h2></section><section>    <header><font color="white">Unified Model</font></header>		<p><br><br><br>The <strong>Unified Model (UM)</strong> is the numerical model developed at the UK MetOffice	for climate and weather research and forecasting.  It contains hundreds of thousands of lines of code.  It is 	comprised of 4 sub-models:</p>	<ol>	   <li> UM - atmosphere model</li>	   <li> NEMO - ocean model</li>	   <li> CICE - sea ice model</li>	   <li> JULES - land processes model</li>	</ol>	<p>Each sub-model is a fully parallel application utilizing domain (and task) decomposition.</p></section><section>      <figure-fullpage><img src='images/climate_model_parts.jpg'></figure-fullpage></section><section>    <header><font color="white">Unified Model</font></header>		<p><br><br><br>One can run 1 or more sub-models for a simulation. For example: </p>	<ul>	   <li> EcoConnect runs use UM+JULES for its weather forecasts</li>	   <li> Olaf Morgenstern uses UM+JULES+NEMO+CICE for his climate runs</li>	</ul>	<p>Running sub-models is an example of task decomposition.  The CPUs allocated to the "full" UM job are	divided between the running sub-models.  This division is usually controlled by input from the user.</p>		<center>	<table style="width:300px; border: 1px solid black; font-size: 25px;">	<tr>	   <td bgcolor="black" colspan="4"><font color="white">UM "Job"</font></td>	</tr>	<tr><td colspan="4">1024 CPUs</td></tr>	<tr>	    <td bgcolor="grey"><font color="white">UM</font></td>		<td bgcolor="grey"><font color="white">JULES</font></td>		<td bgcolor="grey"><font color="white">NEMO</font></td>		<td bgcolor="grey"><font color="white">CICE</font></td>	</tr>	<tr>	    <td>512</td>		<td>64</td>		<td>384</td>		<td>64</td>	</tr>	</table>	</center>	</section><!-- Coupling slides --><section>    <header><font color="white">Purely internal coupling</font></header>		<figure>        <img style="width: 40%; height: 40%; margin-top: 0px;" src='images/purely_internal_coupling.svg'>    </figure>		<p><br><br>Task decomposition could be controlled by IF-THEN calls inside the main code. Information is available and 	readily passed inside the main application.  This is how the UM passes information between the atmosphere 	and land sub-models</p>		<br>	<center>	<table style="width:700px; border: 1px solid black; font-size: 25px;">	<tr>	   <td bgcolor="black"><font color="white">Pros</font></td>	   <td bgcolor="black"><font color="white">Cons</font></td>	</tr>	<tr>	    <td rowspan="2">best performance</td>		<td>more code complexity</td>	</tr>	<tr>		<td>harder to modify code</td>	</tr>	</table>	</center>	</section><section>    <header><font color="white">Internal coupling</font></header>		<figure>        <img style="width: 40%; height: 40%;" src='images/internal_coupling.svg'>    </figure>		<p><br><br><br>Another software library/application performs the information exchange.  Function calls 	are used inside the main application so the passing of information still appears to be	internal.</p>		<br>	<center>	<table style="width:750px; border: 1px solid black; font-size: 25px;">	<tr>	   <td bgcolor="black"><font color="white">Pros</font></td>	   <td bgcolor="black"><font color="white">Cons</font></td>	</tr>	<tr>	    <td>less code complexity</td>		<td>performance dependent on external software</td>	</tr>	<tr>	    <td rowspan="2">more design flexibility</td>		<td>API complexity</td>	</tr>	<tr>		<td>lifespan of external coupler</td>	</tr>	</table>	</center>	</section><section>    <header><font color="white">External coupling</font></header>		<figure>        <img style="width: 40%; height: 40%;" src='images/external_coupling.svg'>    </figure>		<p><br><br><br>Some applications use a "loose-coupling" method.  Usually this involves information being passed by	files.  Eg. Output file of one sub-model is used later as input for another sub-model.</p>		<br>	<center>	<table style="width:600px; border: 1px solid black; font-size: 25px;">	<tr>	   <td bgcolor="black"><font color="white">Pros</font></td>	   <td bgcolor="black"><font color="white">Cons</font></td>	</tr>	<tr>	    <td rowspan="2">easy to implement</td>		<td>performance now I/O dependent</td>	</tr>	<tr>		<td>less flexibility & adaptability</td>	</tr>	</table>	</center>	</section><section>    <h2>Questions?</h2>        <footer> Mark Cheeseman<br>        <font size='-1'>mche807@nesi.org.nz</font>    </footer></section><!-- Your Style --><!-- Define the style of your presentation --><!-- Maybe a font from http://www.google.com/webfonts ? --><link href='http://fonts.googleapis.com/css?family=Oswald' rel='stylesheet'><style>  html, .view body { background-color: black; counter-reset: slideidx; }  body, .view section { background-color: white; border-radius: 12px }  /* A section is a slide. It's size is 800x600, and this will never change */  section, .view head > title {      /* The font from Google */      font-family: 'Oswald', arial, serif;      font-size: 30px;  }  .view section:after {    counter-increment: slideidx;    content: counter(slideidx, decimal-leading-zero);    position: absolute; bottom: -80px; right: 100px;    color: white;  }  .view head > title {    color: white;    text-align: center;    margin: 1em 0 1em 0;  }  h1 {    margin-top: 200px;    text-align: center;    font-size: 80px;  }  h2 {    margin-top: 200px;    text-align: center;    font-size: 60px;  }  h3 {    margin: 100px 0 50px 100px;  }  ul {     font-size: 22px;     margin: 10px 100px 10px;  }  ol {      font-size: 22px;      margin: 10px 100px 10px;  }    dl {      font-size: 22px;      margin: 10px 52px 10px;  }       p {    font-size: 25px;    margin: 20px 60px 10px;  }  code {    font-size: 20px;    margin: 20px 60px 10px;  }     pre {    background-color: #C9CFD6; }     blockquote {    height: 100%;    background-color: black;    color: white;    font-size: 60px;    padding: 50px;  }  blockquote:before {    content: open-quote;  }  blockquote:after {    content: close-quote;  }  /* Figures are displayed full-page, with the caption     on top of the image/video */  figure-bottom {    width: 20%;    height: 20%;    margin-top: 10px;    margin-left: 10px;    margin-right: 0px;    margin-bottom: 5px;    float: right;  }  figure {    width: 40%;    height: 40%;    margin-top: 98px;    margin-left: 0px;    margin-right: 15px;    float: left;  }  figure2 {    margin-top: 125px;    margin-left: 15px;    margin-right: 5px;    float: right;  }   figure2 > img, figure2 > video {    width: 100%; height: 100%;  }    figure > * {    position: absolute;  }  figure > img, figure > video {    width: 40%; height: 40%;  }  figcaption {    margin: 70px 100px;    font-size: 20px;    background: white;  }  figure-fullpage {    background-color: black;    width: 100%;    height: 100%;  }  figure-fullpage > * {    position: absolute;  }  figure-fullpage > img, figure-fullpage > video {    width: 100%; height: 100%;  }  figure-bottom {    width: 30%;    height: 30%;    margin: 10px 10px 10px;  }  figure-bottom > * {    position: absolute;  }  figure-bottom > img, figure-bottom > video {    margin: 10px 10px 10px;    width: 30%; height: 30%;  }  figure-middle > img, figure-middle > video {    margin: 10px 10px;    width: 45%;     height: 45%;  }  figure-middle2 > img, figure-middle > video {    margin: 10px 10px;    width: 30%;     height: 30%;  }  figure2 {    width: 40%;    height: 40%;    margin-top: 98px;    margin-left: 15px;    margin-right: 5px;    float: right;  }    footer {    position: absolute;    bottom: 0;    width: 100%;    padding: 10px;    text-align: right;    background-color: white;    border-top: 2px solid #CCC;    background-image:url(images/NeSI_Logo.jpg);    background-repeat:no-repeat;  }  footer2 {     position: absolute;     bottom: 0;     width: 100%;     padding: 30px;     background-color: white;     border-top: 2px solid #CCC;     background-image:url(images/nesi_collaborator_logos.png);     background-repeat:no-repeat;  }  header {    position: absolute;    bottom: 200;    width: 100%;    padding: 25px;    text-align: left;    text-color: white;    background-color: #25383C;    border-bottom: 2px solid #CCC;    background-image:url(images/NeSI_Logo.jpg);    background-repeat:no-repeat;    background-position:right;    margin: 0px 0px;  }  /* Transition effect */  /* Feel free to change the transition effect for original     animations. See here:     https://developer.mozilla.org/en/CSS/CSS_transitions     How to use CSS3 Transitions: */  section {    -moz-transition: left 400ms linear 0s;    -webkit-transition: left 400ms linear 0s;    -ms-transition: left 400ms linear 0s;    transition: left 400ms linear 0s;  }  .view section {    -moz-transition: none;    -webkit-transition: none;    -ms-transition: none;    transition: none;  }  .view section[aria-selected] {    border: 5px red solid;  }  /* Before */  section { left: -150%; }  /* Now */  section[aria-selected] { left: 0; }  /* After */  section[aria-selected] ~ section { left: +150%; }  /* Incremental elements */  /* By default, visible */  .incremental > * { opacity: 1; }  /* The current item */  .incremental > *[aria-selected] { opacity: 1; }  /* The items to-be-selected */  .incremental > *[aria-selected] ~ * { opacity: 0; }  /* The progressbar, at the bottom of the slides, show the global     progress of the presentation. */  #progress-bar {    height: 2px;    background: #AAA;  }</style><!-- {{{{ dzslides core###     __  __  __       .  __   ___  __#    |  \  / /__` |    | |  \ |__  /__`#    |__/ /_ .__/ |___ | |__/ |___ .__/ core :â‚¬### The following block of code is not supposed to be edited.# But if you want to change the behavior of these slides,# feel free to hack it!#--><div id="progress-bar"></div><!-- Default Style --><style>  * { margin: 0; padding: 0; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; box-sizing: border-box; }  [role="note"] { display: none; }  body {    width: 800px; height: 600px;    margin-left: -400px; margin-top: -300px;    position: absolute; top: 50%; left: 50%;    overflow: hidden;    display: none;  }  .view body {    position: static;    margin: 0; padding: 0;    width: 100%; height: 100%;    display: inline-block;    overflow: visible; overflow-x: hidden;    /* undo Dz.onresize */    transform: none !important;    -moz-transform: none !important;    -webkit-transform: none !important;    -o-transform: none !important;    -ms-transform: none !important;  }  .view head, .view head > title { display: block }  section {    position: absolute;    pointer-events: none;    width: 100%; height: 100%;  }  .view section {    pointer-events: auto;    position: static;    width: 800px; height: 600px;    margin: -150px -200px;    float: left;    transform: scale(.4);    -moz-transform: scale(.4);    -webkit-transform: scale(.4);    -o-transform: scale(.4);    -ms-transform: scale(.4);  }  .view section > * { pointer-events: none; }  section[aria-selected] { pointer-events: auto; }  html { overflow: hidden; }  html.view { overflow: visible; }  body.loaded { display: block; }  .incremental {visibility: hidden; }  .incremental[active] {visibility: visible; }  #progress-bar{    bottom: 0;    position: absolute;    -moz-transition: width 400ms linear 0s;    -webkit-transition: width 400ms linear 0s;    -ms-transition: width 400ms linear 0s;    transition: width 400ms linear 0s;  }  .view #progress-bar {    display: none;  }</style><script>  var Dz = {    remoteWindows: [],    idx: -1,    step: 0,    html: null,    slides: null,    progressBar : null,    params: {      autoplay: "1"    }  };  Dz.init = function() {    document.body.className = "loaded";    this.slides = Array.prototype.slice.call($$("body > section"));    this.progressBar = $("#progress-bar");    this.html = document.body.parentNode;    this.setupParams();    this.onhashchange();    this.setupTouchEvents();    this.onresize();    this.setupView();  }  Dz.setupParams = function() {    var p = window.location.search.substr(1).split('&');    p.forEach(function(e, i, a) {      var keyVal = e.split('=');      Dz.params[keyVal[0]] = decodeURIComponent(keyVal[1]);    });  // Specific params handling    if (!+this.params.autoplay)      $$.forEach($$("video"), function(v){ v.controls = true });  }  Dz.onkeydown = function(aEvent) {    // Don't intercept keyboard shortcuts    if (aEvent.altKey      || aEvent.ctrlKey      || aEvent.metaKey      || aEvent.shiftKey) {      return;    }    if ( aEvent.keyCode == 37 // left arrow      || aEvent.keyCode == 38 // up arrow      || aEvent.keyCode == 33 // page up    ) {      aEvent.preventDefault();      this.back();    }    if ( aEvent.keyCode == 39 // right arrow      || aEvent.keyCode == 40 // down arrow      || aEvent.keyCode == 34 // page down    ) {      aEvent.preventDefault();      this.forward();    }    if (aEvent.keyCode == 35) { // end      aEvent.preventDefault();      this.goEnd();    }    if (aEvent.keyCode == 36) { // home      aEvent.preventDefault();      this.goStart();    }    if (aEvent.keyCode == 32) { // space      aEvent.preventDefault();      this.toggleContent();    }    if (aEvent.keyCode == 70) { // f      aEvent.preventDefault();      this.goFullscreen();    }    if (aEvent.keyCode == 79) { // o      aEvent.preventDefault();      this.toggleView();    }  }  /* Touch Events */  Dz.setupTouchEvents = function() {    var orgX, newX;    var tracking = false;    var db = document.body;    db.addEventListener("touchstart", start.bind(this), false);    db.addEventListener("touchmove", move.bind(this), false);    function start(aEvent) {      aEvent.preventDefault();      tracking = true;      orgX = aEvent.changedTouches[0].pageX;    }    function move(aEvent) {      if (!tracking) return;      newX = aEvent.changedTouches[0].pageX;      if (orgX - newX > 100) {        tracking = false;        this.forward();      } else {        if (orgX - newX < -100) {          tracking = false;          this.back();        }      }    }  }  Dz.setupView = function() {    document.body.addEventListener("click", function ( e ) {      if (!Dz.html.classList.contains("view")) return;      if (!e.target || e.target.nodeName != "SECTION") return;      Dz.html.classList.remove("view");      Dz.setCursor(Dz.slides.indexOf(e.target) + 1);    }, false);  }  /* Adapt the size of the slides to the window */  Dz.onresize = function() {    var db = document.body;    var sx = db.clientWidth / window.innerWidth;    var sy = db.clientHeight / window.innerHeight;    var transform = "scale(" + (1/Math.max(sx, sy)) + ")";    db.style.MozTransform = transform;    db.style.WebkitTransform = transform;    db.style.OTransform = transform;    db.style.msTransform = transform;    db.style.transform = transform;  }  Dz.getNotes = function(aIdx) {    var s = $("section:nth-of-type(" + aIdx + ")");    var d = s.$("[role='note']");    return d ? d.innerHTML : "";  }  Dz.onmessage = function(aEvent) {    var argv = aEvent.data.split(" "), argc = argv.length;    argv.forEach(function(e, i, a) { a[i] = decodeURIComponent(e) });    var win = aEvent.source;    if (argv[0] === "REGISTER" && argc === 1) {      this.remoteWindows.push(win);      this.postMsg(win, "REGISTERED", document.title, this.slides.length);      this.postMsg(win, "CURSOR", this.idx + "." + this.step);      return;    }    if (argv[0] === "BACK" && argc === 1)      this.back();    if (argv[0] === "FORWARD" && argc === 1)      this.forward();    if (argv[0] === "START" && argc === 1)      this.goStart();    if (argv[0] === "END" && argc === 1)      this.goEnd();    if (argv[0] === "TOGGLE_CONTENT" && argc === 1)      this.toggleContent();    if (argv[0] === "SET_CURSOR" && argc === 2)      window.location.hash = "#" + argv[1];    if (argv[0] === "GET_CURSOR" && argc === 1)      this.postMsg(win, "CURSOR", this.idx + "." + this.step);    if (argv[0] === "GET_NOTES" && argc === 1)      this.postMsg(win, "NOTES", this.getNotes(this.idx));  }  Dz.toggleContent = function() {    // If a Video is present in this new slide, play it.    // If a Video is present in the previous slide, stop it.    var s = $("section[aria-selected]");    if (s) {      var video = s.$("video");      if (video) {        if (video.ended || video.paused) {          video.play();        } else {          video.pause();        }      }    }  }  Dz.setCursor = function(aIdx, aStep) {    // If the user change the slide number in the URL bar, jump    // to this slide.    aStep = (aStep != 0 && typeof aStep !== "undefined") ? "." + aStep : ".0";    window.location.hash = "#" + aIdx + aStep;  }  Dz.onhashchange = function() {    var cursor = window.location.hash.split("#"),        newidx = 1,        newstep = 0;    if (cursor.length == 2) {      newidx = ~~cursor[1].split(".")[0];      newstep = ~~cursor[1].split(".")[1];      if (newstep > Dz.slides[newidx - 1].$$('.incremental > *').length) {        newstep = 0;        newidx++;      }    }    this.setProgress(newidx, newstep);    if (newidx != this.idx) {      this.setSlide(newidx);    }    if (newstep != this.step) {      this.setIncremental(newstep);    }    for (var i = 0; i < this.remoteWindows.length; i++) {      this.postMsg(this.remoteWindows[i], "CURSOR", this.idx + "." + this.step);    }  }  Dz.back = function() {    if (this.idx == 1 && this.step == 0) {      return;    }    if (this.step == 0) {      this.setCursor(this.idx - 1,                     this.slides[this.idx - 2].$$('.incremental > *').length);    } else {      this.setCursor(this.idx, this.step - 1);    }  }  Dz.forward = function() {    if (this.idx >= this.slides.length &&        this.step >= this.slides[this.idx - 1].$$('.incremental > *').length) {        return;    }    if (this.step >= this.slides[this.idx - 1].$$('.incremental > *').length) {      this.setCursor(this.idx + 1, 0);    } else {      this.setCursor(this.idx, this.step + 1);    }  }  Dz.goStart = function() {    this.setCursor(1, 0);  }  Dz.goEnd = function() {    var lastIdx = this.slides.length;    var lastStep = this.slides[lastIdx - 1].$$('.incremental > *').length;    this.setCursor(lastIdx, lastStep);  }  Dz.toggleView = function() {    this.html.classList.toggle("view");    if (this.html.classList.contains("view")) {      $("section[aria-selected]").scrollIntoView(true);    }  }  Dz.setSlide = function(aIdx) {    this.idx = aIdx;    var old = $("section[aria-selected]");    var next = $("section:nth-of-type("+ this.idx +")");    if (old) {      old.removeAttribute("aria-selected");      var video = old.$("video");      if (video) {        video.pause();      }    }    if (next) {      next.setAttribute("aria-selected", "true");      if (this.html.classList.contains("view")) {        next.scrollIntoView();      }      var video = next.$("video");      if (video && !!+this.params.autoplay) {        video.play();      }    } else {      // That should not happen      this.idx = -1;      // console.warn("Slide doesn't exist.");    }  }  Dz.setIncremental = function(aStep) {    this.step = aStep;    var old = this.slides[this.idx - 1].$('.incremental > *[aria-selected]');    if (old) {      old.removeAttribute('aria-selected');    }    var incrementals = $$('.incremental');    if (this.step <= 0) {      $$.forEach(incrementals, function(aNode) {        aNode.removeAttribute('active');      });      return;    }    var next = this.slides[this.idx - 1].$$('.incremental > *')[this.step - 1];    if (next) {      next.setAttribute('aria-selected', true);      next.parentNode.setAttribute('active', true);      var found = false;      $$.forEach(incrementals, function(aNode) {        if (aNode != next.parentNode)          if (found)            aNode.removeAttribute('active');          else            aNode.setAttribute('active', true);        else          found = true;      });    } else {      setCursor(this.idx, 0);    }    return next;  }  Dz.goFullscreen = function() {    var html = $('html'),        requestFullscreen = html.requestFullscreen || html.requestFullScreen || html.mozRequestFullScreen || html.webkitRequestFullScreen;    if (requestFullscreen) {      requestFullscreen.apply(html);    }  }    Dz.setProgress = function(aIdx, aStep) {    var slide = $("section:nth-of-type("+ aIdx +")");    if (!slide)      return;    var steps = slide.$$('.incremental > *').length + 1,        slideSize = 100 / (this.slides.length - 1),        stepSize = slideSize / steps;    this.progressBar.style.width = ((aIdx - 1) * slideSize + aStep * stepSize) + '%';  }    Dz.postMsg = function(aWin, aMsg) { // [arg0, [arg1...]]    aMsg = [aMsg];    for (var i = 2; i < arguments.length; i++)      aMsg.push(encodeURIComponent(arguments[i]));    aWin.postMessage(aMsg.join(" "), "*");  }    function init() {    Dz.init();    window.onkeydown = Dz.onkeydown.bind(Dz);    window.onresize = Dz.onresize.bind(Dz);    window.onhashchange = Dz.onhashchange.bind(Dz);    window.onmessage = Dz.onmessage.bind(Dz);  }  window.onload = init;</script><script> // Helpers  if (!Function.prototype.bind) {    Function.prototype.bind = function (oThis) {      // closest thing possible to the ECMAScript 5 internal IsCallable      // function       if (typeof this !== "function")      throw new TypeError(        "Function.prototype.bind - what is trying to be fBound is not callable"      );      var aArgs = Array.prototype.slice.call(arguments, 1),          fToBind = this,          fNOP = function () {},          fBound = function () {            return fToBind.apply( this instanceof fNOP ? this : oThis || window,                   aArgs.concat(Array.prototype.slice.call(arguments)));          };      fNOP.prototype = this.prototype;      fBound.prototype = new fNOP();      return fBound;    };  }  var $ = (HTMLElement.prototype.$ = function(aQuery) {    return this.querySelector(aQuery);  }).bind(document);  var $$ = (HTMLElement.prototype.$$ = function(aQuery) {    return this.querySelectorAll(aQuery);  }).bind(document);  $$.forEach = function(nodeList, fun) {    Array.prototype.forEach.call(nodeList, fun);  }</script><!-- vim: set fdm=marker: }}} -->